{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Explore Dataset\n",
        "\n",
        "This notebook explores the DFG classification dataset:\n",
        "- Dataset statistics\n",
        "- Class distribution visualization\n",
        "- Data quality checks\n",
        "- Sample data inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path().absolute().parent / 'src'))\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úì Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Data path found: ../dfg-classifier/data/processed\n",
            "  Available files: dataset_stats.json, test.json, train.json, val.json\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_PATH = '../dfg-classifier/data/processed'  # Change to '../dfg-classifier/data/small' for small dataset\n",
        "DFG_MAPPING_PATH = '../data/dfg_mapping.json'\n",
        "\n",
        "# Check if data exists\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"‚ö†Ô∏è  Data path not found: {DATA_PATH}\")\n",
        "    print(\"üí° Try changing DATA_PATH to '../dfg-classifier/data/small' for small dataset\")\n",
        "else:\n",
        "    print(f\"‚úì Data path found: {DATA_PATH}\")\n",
        "    \n",
        "    # List available files\n",
        "    if os.path.isdir(DATA_PATH):\n",
        "        files = os.listdir(DATA_PATH)\n",
        "        print(f\"  Available files: {', '.join(files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö DFG Classification Structure:\n",
            "  Total Review Boards: 218\n",
            "  Classification Levels: 4\n",
            "\n",
            "Main Disciplines (Level 1):\n",
            "  1: Humanities & Social Sciences\n",
            "  2: Life Sciences\n",
            "  3: Natural Sciences\n",
            "  4: Engineering Sciences\n",
            "\n",
            "Subject Areas (Level 2): 30 classes\n"
          ]
        }
      ],
      "source": [
        "# Load DFG mapping\n",
        "with open(DFG_MAPPING_PATH, 'r', encoding='utf-8') as f:\n",
        "    dfg_mapping = json.load(f)\n",
        "\n",
        "# Display DFG structure\n",
        "print(\"üìö DFG Classification Structure:\")\n",
        "print(f\"  Total Review Boards: {dfg_mapping['metadata']['total_review_boards']}\")\n",
        "print(f\"  Classification Levels: {dfg_mapping['metadata']['total_levels']}\")\n",
        "print(\"\\nMain Disciplines (Level 1):\")\n",
        "for code, name in dfg_mapping['level_1']['classes'].items():\n",
        "    print(f\"  {code}: {name}\")\n",
        "\n",
        "# Get all level 2 classes\n",
        "level_2_classes = dfg_mapping['level_2']['classes']\n",
        "print(f\"\\nSubject Areas (Level 2): {len(level_2_classes)} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded train set: 4800 samples\n",
            "‚úì Loaded val set: 600 samples\n",
            "‚úì Loaded test set: 600 samples\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_split(split='train'):\n",
        "    \"\"\"Load a dataset split\"\"\"\n",
        "    file_path = os.path.join(DATA_PATH, f'{split}.json')\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
        "        return None\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    print(f\"‚úì Loaded {split} set: {len(data)} samples\")\n",
        "    return data\n",
        "\n",
        "# Load all splits\n",
        "train_data = load_dataset_split('train')\n",
        "val_data = load_dataset_split('val')\n",
        "test_data = load_dataset_split('test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Statistics\n",
        "if train_data:\n",
        "    total_samples = len(train_data) + (len(val_data) if val_data else 0) + (len(test_data) if test_data else 0)\n",
        "    \n",
        "    stats = {\n",
        "        'Total Samples': total_samples,\n",
        "        'Train Samples': len(train_data) if train_data else 0,\n",
        "        'Val Samples': len(val_data) if val_data else 0,\n",
        "        'Test Samples': len(test_data) if test_data else 0,\n",
        "    }\n",
        "    \n",
        "    if train_data:\n",
        "        stats['Train %'] = f\"{len(train_data)/total_samples*100:.1f}%\"\n",
        "    if val_data:\n",
        "        stats['Val %'] = f\"{len(val_data)/total_samples*100:.1f}%\"\n",
        "    if test_data:\n",
        "        stats['Test %'] = f\"{len(test_data)/total_samples*100:.1f}%\"\n",
        "    \n",
        "    print(\"üìä Dataset Statistics:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Distribution Analysis\n",
        "if train_data:\n",
        "    # Count labels in each split\n",
        "    train_labels = [item['label'] for item in train_data]\n",
        "    val_labels = [item['label'] for item in val_data] if val_data else []\n",
        "    test_labels = [item['label'] for item in test_data] if test_data else []\n",
        "    \n",
        "    train_counter = Counter(train_labels)\n",
        "    val_counter = Counter(val_labels) if val_labels else Counter()\n",
        "    test_counter = Counter(test_labels) if test_labels else Counter()\n",
        "    \n",
        "    # Get all unique labels\n",
        "    all_labels = sorted(set(train_labels + val_labels + test_labels))\n",
        "    \n",
        "    print(f\"üìà Class Distribution:\")\n",
        "    print(f\"  Total unique classes: {len(all_labels)}\")\n",
        "    print(f\"  Classes in train set: {len(train_counter)}\")\n",
        "    print(f\"  Classes in val set: {len(val_counter)}\")\n",
        "    print(f\"  Classes in test set: {len(test_counter)}\")\n",
        "    \n",
        "    # Show top 10 most common classes in train set\n",
        "    print(\"\\nüèÜ Top 10 Most Common Classes (Train Set):\")\n",
        "    for label, count in train_counter.most_common(10):\n",
        "        print(f\"  {label}: {count} samples\")\n",
        "    \n",
        "    # Show least common classes\n",
        "    print(\"\\nüìâ Least Common Classes (Train Set):\")\n",
        "    for label, count in train_counter.most_common()[-10:]:\n",
        "        print(f\"  {label}: {count} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Class Distribution\n",
        "if train_data:\n",
        "    # Prepare data for visualization\n",
        "    label_counts = pd.DataFrame([\n",
        "        {'Split': 'Train', 'Label': label, 'Count': count}\n",
        "        for label, count in train_counter.items()\n",
        "    ] + ([\n",
        "        {'Split': 'Val', 'Label': label, 'Count': count}\n",
        "        for label, count in val_counter.items()\n",
        "    ] if val_data else []) + ([\n",
        "        {'Split': 'Test', 'Label': label, 'Count': count}\n",
        "        for label, count in test_counter.items()\n",
        "    ] if test_data else []))\n",
        "    \n",
        "    # Plot class distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Bar plot of top classes\n",
        "    top_labels = train_counter.most_common(20)\n",
        "    labels, counts = zip(*top_labels)\n",
        "    \n",
        "    axes[0].barh(range(len(labels)), counts, color='steelblue')\n",
        "    axes[0].set_yticks(range(len(labels)))\n",
        "    axes[0].set_yticklabels(labels, fontsize=8)\n",
        "    axes[0].set_xlabel('Number of Samples', fontsize=12)\n",
        "    axes[0].set_title('Top 20 Classes by Sample Count (Train Set)', fontsize=14, fontweight='bold')\n",
        "    axes[0].invert_yaxis()\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Distribution histogram\n",
        "    counts_list = list(train_counter.values())\n",
        "    axes[1].hist(counts_list, bins=20, color='coral', edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_xlabel('Samples per Class', fontsize=12)\n",
        "    axes[1].set_ylabel('Number of Classes', fontsize=12)\n",
        "    axes[1].set_title('Distribution of Samples per Class', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Statistics\n",
        "    print(f\"\\nüìä Class Distribution Statistics:\")\n",
        "    print(f\"  Mean samples per class: {np.mean(counts_list):.1f}\")\n",
        "    print(f\"  Median samples per class: {np.median(counts_list):.1f}\")\n",
        "    print(f\"  Std samples per class: {np.std(counts_list):.1f}\")\n",
        "    print(f\"  Min samples per class: {np.min(counts_list)}\")\n",
        "    print(f\"  Max samples per class: {np.max(counts_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Checks\n",
        "if train_data:\n",
        "    # Check for missing data\n",
        "    print(\"üîç Data Quality Checks:\")\n",
        "    \n",
        "    # Check text lengths\n",
        "    title_lengths = [len(item.get('title', '')) for item in train_data]\n",
        "    abstract_lengths = [len(item.get('abstract', '')) for item in train_data]\n",
        "    combined_lengths = [title_len + abstract_len for title_len, abstract_len in zip(title_lengths, abstract_lengths)]\n",
        "    \n",
        "    # Check for empty fields\n",
        "    empty_titles = sum(1 for item in train_data if not item.get('title', '').strip())\n",
        "    empty_abstracts = sum(1 for item in train_data if not item.get('abstract', '').strip())\n",
        "    missing_labels = sum(1 for item in train_data if not item.get('label', ''))\n",
        "    \n",
        "    print(f\"  Empty titles: {empty_titles}\")\n",
        "    print(f\"  Empty abstracts: {empty_abstracts}\")\n",
        "    print(f\"  Missing labels: {missing_labels}\")\n",
        "    \n",
        "    # Text length statistics\n",
        "    print(f\"\\nüìù Text Length Statistics:\")\n",
        "    print(f\"  Title length - Mean: {np.mean(title_lengths):.1f}, Median: {np.median(title_lengths):.1f}\")\n",
        "    print(f\"  Abstract length - Mean: {np.mean(abstract_lengths):.1f}, Median: {np.median(abstract_lengths):.1f}\")\n",
        "    print(f\"  Combined length - Mean: {np.mean(combined_lengths):.1f}, Median: {np.median(combined_lengths):.1f}\")\n",
        "    \n",
        "    # Visualize text lengths\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    axes[0].hist(title_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0].set_xlabel('Title Length (characters)', fontsize=11)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
        "    axes[0].set_title('Title Length Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    axes[1].hist(abstract_lengths, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_xlabel('Abstract Length (characters)', fontsize=11)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
        "    axes[1].set_title('Abstract Length Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    axes[2].hist(combined_lengths, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
        "    axes[2].set_xlabel('Combined Length (characters)', fontsize=11)\n",
        "    axes[2].set_ylabel('Frequency', fontsize=11)\n",
        "    axes[2].set_title('Combined Text Length Distribution', fontsize=13, fontweight='bold')\n",
        "    axes[2].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Data Inspection\n",
        "if train_data:\n",
        "    print(\"üìñ Sample Data Examples:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show a few random samples\n",
        "    import random\n",
        "    sample_indices = random.sample(range(len(train_data)), min(3, len(train_data)))\n",
        "    \n",
        "    for idx, sample_idx in enumerate(sample_indices, 1):\n",
        "        sample = train_data[sample_idx]\n",
        "        print(f\"\\n[Sample {idx}]\")\n",
        "        print(f\"  Filename: {sample.get('filename', 'N/A')}\")\n",
        "        print(f\"  Label: {sample.get('label', 'N/A')}\")\n",
        "        print(f\"  Title: {sample.get('title', 'N/A')[:100]}...\" if len(sample.get('title', '')) > 100 else f\"  Title: {sample.get('title', 'N/A')}\")\n",
        "        print(f\"  Abstract: {sample.get('abstract', 'N/A')[:200]}...\" if len(sample.get('abstract', '')) > 200 else f\"  Abstract: {sample.get('abstract', 'N/A')}\")\n",
        "        print(f\"  Input IDs shape: {len(sample.get('input_ids', []))} tokens\")\n",
        "        print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset statistics file if available\n",
        "stats_file = os.path.join(DATA_PATH, 'dataset_stats.json')\n",
        "if os.path.exists(stats_file):\n",
        "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
        "        dataset_stats = json.load(f)\n",
        "    \n",
        "    print(\"üìã Saved Dataset Statistics:\")\n",
        "    print(json.dumps(dataset_stats, indent=2))\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No dataset_stats.json file found\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
